{
 "cells": [
  {
   "cell_type": "raw",
   "id": "17c61d8c-f77e-4646-a4db-9ffa136a95e1",
   "metadata": {},
   "source": [
    "# Statistics Part 2 Assignment\n",
    "\n",
    "## Theory Questions\n",
    "\n",
    "# 1. What is hypothesis testing in statistics?\n",
    "# **Answer**: Hypothesis testing is a statistical method used to determine if there is enough evidence in a sample of data to support a specific hypothesis about a population parameter. It involves formulating a null hypothesis (H₀) and an alternative hypothesis (H₁), then using sample data to assess the validity of H₀. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 2. What is the null hypothesis, and how does it differ from the alternative hypothesis?\n",
    "# **Answer**: The null hypothesis (H₀) is a statement that there is no effect or no difference in the population; it serves as the default assumption. The alternative hypothesis (H₁) posits that there is an effect or a difference. The goal of hypothesis testing is to determine whether there is sufficient evidence to reject H₀ in favor of H₁. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# 3. What is the significance level in hypothesis testing, and why is it important?\n",
    "# **Answer**: The significance level (α) is the probability of rejecting the null hypothesis when it is actually true, commonly set at 0.05. It represents the threshold for determining statistical significance and helps control the risk of Type I errors. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# 4. What does a P-value represent in hypothesis testing?\n",
    "# **Answer**: The P-value is the probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. A smaller P-value indicates stronger evidence against H₀. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "# 5. How do you interpret the P-value in hypothesis testing?\n",
    "# **Answer**: If the P-value is less than or equal to the significance level (α), we reject the null hypothesis, suggesting that the observed data is unlikely under H₀. If the P-value is greater than α, we fail to reject H₀, indicating insufficient evidence to support H₁. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# 6. What are Type 1 and Type 2 errors in hypothesis testing?\n",
    "# **Answer**: A Type I error occurs when the null hypothesis is incorrectly rejected (false positive). A Type II error occurs when the null hypothesis is incorrectly not rejected (false negative). :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "# 7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n",
    "# **Answer**: A one-tailed test assesses the possibility of an effect in one direction (either greater than or less than a certain value), while a two-tailed test assesses the possibility of an effect in both directions. :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "# 8. What is the Z-test, and when is it used in hypothesis testing?\n",
    "# **Answer**: The Z-test is a statistical test used to determine if there is a significant difference between sample and population means or between sample means, assuming the data is normally distributed and the population variance is known. :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "# 9. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
    "# **Answer**: The Z-score is calculated by subtracting the population mean from the sample mean and dividing by the population standard deviation. It represents how many standard deviations the sample mean is from the population mean. :contentReference[oaicite:8]{index=8}\n",
    "\n",
    "# 10. What is the T-distribution, and when should it be used instead of the normal distribution?\n",
    "# **Answer**: The T-distribution is a probability distribution used in hypothesis testing when the sample size is small and the population standard deviation is unknown. It accounts for the increased variability expected in small samples. :contentReference[oaicite:9]{index=9}\n",
    "\n",
    "# 11. What is the difference between a Z-test and a T-test?\n",
    "# **Answer**: The Z-test is used when the population variance is known and the sample size is large (typically n > 30). It assumes that the data follows a normal distribution. The T-test is used when the population variance is unknown and the sample size is small (typically n < 30). It also assumes that the data follows a normal distribution. The T-distribution is used in the T-test to account for the additional uncertainty in estimating the population variance from a small sample. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 12. What is the T-test, and how is it used in hypothesis testing?\n",
    "# **Answer**: The T-test is a statistical test used to determine if there is a significant difference between the means of two groups. It is commonly used when the sample size is small and the population variance is unknown. The T-test compares the observed difference between sample means to the variability within the samples to assess whether the observed difference is statistically significant. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# 13. What is the relationship between Z-test and T-test in hypothesis testing?\n",
    "# **Answer**: Both the Z-test and T-test are used to test hypotheses about population means. The Z-test is appropriate when the population variance is known and the sample size is large, while the T-test is used when the population variance is unknown and the sample size is small. As the sample size increases, the T-distribution approaches the normal distribution, making the T-test results similar to those of the Z-test. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# 14. What is a confidence interval, and how is it used to interpret statistical results?\n",
    "# **Answer**: A confidence interval is a range of values, derived from sample data, that is used to estimate an unknown population parameter. It provides a range within which the true parameter value is expected to lie with a certain level of confidence (e.g., 95%). Confidence intervals are used to assess the precision and reliability of sample estimates. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "# 15. What is the margin of error, and how does it affect the confidence interval?\n",
    "# **Answer**: The margin of error is the amount by which the sample estimate may differ from the true population parameter. It is influenced by the sample size, variability in the data, and the desired confidence level. A larger margin of error indicates less precision in the estimate, resulting in a wider confidence interval. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# 16. How is Bayes' Theorem used in statistics, and what is its significance?\n",
    "# **Answer**: Bayes' Theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence. In statistics, it is used to revise the probability of a hypothesis as more data becomes available. Bayes' Theorem is significant because it provides a framework for incorporating prior knowledge and updating beliefs in light of new data. :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "# 17. What is the Chi-square distribution, and when is it used?\n",
    "# **Answer**: The Chi-square distribution is a probability distribution that arises in statistics when analyzing categorical data. It is used in tests of independence and goodness of fit to assess whether observed frequencies differ significantly from expected frequencies. The Chi-square distribution is used when the data consists of counts or frequencies in categories. :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "# 18. What is the Chi-square goodness of fit test, and how is it applied?\n",
    "# **Answer**: The Chi-square goodness of fit test is a statistical test used to determine if a sample data matches an expected distribution. It compares the observed frequencies of events to the expected frequencies under a specific hypothesis. This test is applied when you have categorical data and want to see if the distribution of your sample data fits a particular theoretical distribution. :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "# 19. What is the F-distribution, and when is it used in hypothesis testing?\n",
    "# **Answer**: The F-distribution is a probability distribution that arises in the context of variance analysis. It is used in hypothesis testing to compare two variances and is the basis for the F-test. The F-distribution is used when comparing the variances of two populations or when testing the overall significance of a regression model. :contentReference[oaicite:8]{index=8}\n",
    "\n",
    "# 20. What is an ANOVA test, and what are its assumptions?\n",
    "# **Answer**: An Analysis of Variance (ANOVA) test is a statistical method used to compare means across multiple groups to determine if at least one group mean is different from the others. The assumptions of ANOVA include independence of observations, normality of the data within each group, and homogeneity of variances (equal variances across groups). :contentReference[oaicite:9]{index=9}\n",
    "\n",
    "# 21. What are the different types of ANOVA tests?\n",
    "# **Answer**: The main types of ANOVA tests are:\n",
    "# - **One-way ANOVA**: Used when comparing means across multiple groups based on one independent variable.\n",
    "# - **Two-way ANOVA**: Used when comparing means across multiple groups based on two independent variables, and it can also assess the interaction between these variables.\n",
    "# - **Repeated measures ANOVA**: Used when the same subjects are used for each treatment (i.e., repeated measurements are taken on the same subjects). :contentReference[oaicite:10]{index=10}\n",
    "\n",
    "# 22. What is the F-test, and how does it relate to hypothesis testing?\n",
    "# **Answer**: The F-test is a statistical test used to compare two variances to determine if they are significantly different. It is used in the context of ANOVA to test the null hypothesis that all group means are equal. The F-test assesses whether the variability between group means is greater than the variability within groups, indicating a significant effect. :contentReference[oaicite:11]{index=11}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91883a65-4c36-4f41-9326-f7926e838a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac6b70-5840-4309-a3ed-19ea2027e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# Sample data\n",
    "sample_data = np.array([100, 102, 98, 101, 99, 100, 101, 102, 100, 99])\n",
    "population_mean = 100\n",
    "population_std = 2\n",
    "sample_size = len(sample_data)\n",
    "\n",
    "# Perform Z-test\n",
    "z_stat, p_value = ztest(sample_data, value=population_mean, alternative='two-sided', usevar='pooled')\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The sample mean is significantly different from the population mean.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "# 2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "sample_data = np.random.normal(loc=100, scale=15, size=30)\n",
    "population_mean = 100\n",
    "\n",
    "# Perform one-sample t-test\n",
    "t_stat, p_value = stats.ttest_1samp(sample_data, population_mean)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The sample mean is significantly different from the population mean.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# Sample data\n",
    "sample_data = np.array([100, 102, 98, 101, 99, 100, 101, 102, 100, 99])\n",
    "population_mean = 100\n",
    "\n",
    "# Perform Z-test\n",
    "z_stat, p_value = ztest(sample_data, value=population_mean, alternative='two-sided', usevar='pooled')\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The sample mean is significantly different from the population mean.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# Sample data\n",
    "sample_data = np.array([100, 102, 98, 101, 99, 100, 101, 102, 100, 99])\n",
    "population_mean = 100\n",
    "population_std = 2\n",
    "sample_size = len(sample_data)\n",
    "\n",
    "# Perform Z-test\n",
    "z_stat, p_value = ztest(sample_data, value=population_mean, alternative='two-sided', usevar='pooled')\n",
    "\n",
    "# Plot\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = (1/np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)\n",
    "plt.plot(x, y, label='Standard Normal Distribution')\n",
    "plt.fill_between(x, y, where=(x < -1.96) | (x > 1.96), color='red', alpha=0.5, label='Rejection Region')\n",
    "plt.axvline(x=z_stat, color='black', linestyle='--', label=f'Z-statistic: {z_stat:.2f}')\n",
    "plt.title('Two-Tailed Z-Test')\n",
    "plt.xlabel('Z-Score')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The sample mean is significantly different from the population mean.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def visualize_errors(pop_mean, sample_mean, std_dev, sample_size, alpha=0.05):\n",
    "    # Calculate standard error\n",
    "    std_error = std_dev / np.sqrt(sample_size)\n",
    "    \n",
    "    # Critical value for Type 1 error (alpha)\n",
    "    critical_value = norm.ppf(1 - alpha)\n",
    "    \n",
    "    # Calculate Type 2 error (beta)\n",
    "    beta = norm.cdf(critical_value - (sample_mean - pop_mean) / std_error)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.linspace(pop_mean - 4*std_error, pop_mean + 4*std_error, 1000)\n",
    "    y = norm.pdf(x, loc=pop_mean, scale=std_error)\n",
    "    plt.plot(x, y, label='Sampling Distribution under H0')\n",
    "    plt.fill_between(x, y, where=(x > critical_value), color='red', alpha=0.5, label='Type 1 Error Region')\n",
    "    plt.axvline(x=critical_value, color='black', linestyle='--', label=f'Critical Value: {critical_value:.2f}')\n",
    "    plt.title('Type 1 and Type 2 Errors')\n",
    "    plt.xlabel('Sample Mean')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return beta\n",
    "\n",
    "# Example usage\n",
    "pop_mean = 100\n",
    "sample_mean = 102\n",
    "std_dev = 15\n",
    "sample_size = 30\n",
    "alpha = 0.05\n",
    "\n",
    "beta = visualize_errors(pop_mean, sample_mean, std_dev, sample_size, alpha)\n",
    "f\"Type 2 Error Probability (Beta): {beta:.2f}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Write a Python program to perform an independent T-test and interpret the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "group1 = np.array([100, 102, 98, 101, 99, 100, 101, 102, 100, 99])\n",
    "group2 = np.array([105, 107, 103, 106, 104, 105, 106, 107, 105, 104])\n",
    "\n",
    "# Perform independent T-test\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The means of the two groups are significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The means of the two groups are not significantly different.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "# 7. Perform a paired sample T-test using Python and visualize the comparison results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "before_treatment = np.array([88, 82, 84, 93, 75, 78, 84, 87, 95, 91, 83, 89, 77, 68, 91])\n",
    "after_treatment = np.array([91, 84, 88, 90, 79, 80, 88, 90, 90, 96, 88, 89, 81, 74, 92])\n",
    "\n",
    "# Perform paired sample T-test\n",
    "t_stat, p_value = stats.ttest_rel(before_treatment, after_treatment)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The means are significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The means are not significantly different.\"\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(before_treatment, label='Before Treatment', marker='o')\n",
    "plt.plot(after_treatment, label='After Treatment', marker='o')\n",
    "plt.title('Before and After Treatment Scores')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Scores')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 8. Simulate data and perform both Z-test and T-test, then compare the results using Python\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "sample_data = np.random.normal(loc=100, scale=15, size=30)\n",
    "population_mean = 100\n",
    "population_std = 15\n",
    "\n",
    "# Perform one-sample Z-test\n",
    "z_stat = (np.mean(sample_data) - population_mean) / (population_std / np.sqrt(len(sample_data)))\n",
    "p_value_z = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "\n",
    "# Perform one-sample T-test\n",
    "t_stat, p_value_t = stats.ttest_1samp(sample_data, population_mean)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "result_z = \"Reject the null hypothesis: The sample mean is significantly different from the population mean.\" if p_value_z < alpha else \"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\"\n",
    "result_t = \"Reject the null hypothesis: The sample mean is significantly different from the population mean.\" if p_value_t < alpha else \"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\"\n",
    "\n",
    "result_z, result_t\n",
    "\n",
    "\n",
    "\n",
    "# 9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a sample mean.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): Sample data\n",
    "    confidence (float): Confidence level (default is 0.95)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Lower and upper bounds of the confidence interval\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)\n",
    "    margin_of_error = sem * stats.t.ppf((1 + confidence) / 2., len(data)-1)\n",
    "    return mean - margin_of_error, mean + margin_of_error\n",
    "\n",
    "# Example usage\n",
    "sample_data = [100, 102, 98, 101, 99, 100, 101, 102, 100, 99]\n",
    "ci_lower, ci_upper = confidence_interval(sample_data)\n",
    "f\"Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\"\n",
    "\n",
    "\n",
    "\n",
    "# 10. Write a Python program to calculate the margin of error for a given confidence level using sample data\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def margin_of_error(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the margin of error for a sample mean.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): Sample data\n",
    "    confidence (float): Confidence level (default is 0.95)\n",
    "    \n",
    "    Returns:\n",
    "    float: Margin of error\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    sem = stats.sem(data)\n",
    "    return sem * stats.t.ppf((1 + confidence) / 2., len(data)-1)\n",
    "\n",
    "# Example usage\n",
    "sample_data = [100, 102, 98, 101, 99, 100, 101, 102, 100, 99]\n",
    "moe = margin_of_error(sample_data)\n",
    "f\"Margin of Error: {moe:.2f}\"\n",
    "\n",
    "# 11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process\n",
    "# **Answer**:\n",
    "# Bayes' Theorem: P(A|B) = P(B|A) * P(A) / P(B)\n",
    "# Example: Probability of having a disease given a positive test result\n",
    "\n",
    "# Given probabilities\n",
    "P_A = 0.01  # Probability of having the disease (prior)\n",
    "P_B_given_A = 0.95  # Probability of testing positive given disease (likelihood)\n",
    "P_B_given_not_A = 0.05  # Probability of testing positive without disease (false positive rate)\n",
    "\n",
    "# Calculate P(B) - Total probability of testing positive\n",
    "P_B = P_A * P_B_given_A + (1 - P_A) * P_B_given_not_A\n",
    "\n",
    "# Apply Bayes' Theorem\n",
    "P_A_given_B = (P_B_given_A * P_A) / P_B\n",
    "\n",
    "f\"Probability of having the disease given a positive test result: {P_A_given_B:.2f}\"\n",
    "\n",
    "\n",
    "\n",
    "# 12. Perform a Chi-square test for independence between two categorical variables in Python\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: Contingency table\n",
    "data = np.array([[30, 10], [20, 40]])\n",
    "\n",
    "# Perform Chi-square test\n",
    "chi2_stat, p_value, dof, expected = stats.chi2_contingency(data)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The variables are dependent.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The variables are independent.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 13. Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: Contingency table\n",
    "observed = np.array([[30, 10], [20, 40]])\n",
    "\n",
    "# Calculate row and column totals\n",
    "row_totals = observed.sum(axis=1)\n",
    "col_totals = observed.sum(axis=0)\n",
    "grand_total = observed.sum()\n",
    "\n",
    "# Calculate expected frequencies\n",
    "expected = np.outer(row_totals, col_totals) / grand_total\n",
    "\n",
    "expected\n",
    "\n",
    "\n",
    "# 14. Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Observed data\n",
    "observed = np.array([50, 60, 40, 47, 53])\n",
    "\n",
    "# Expected data (e.g., uniform distribution)\n",
    "expected = np.array([50, 50, 50, 50, 50])\n",
    "\n",
    "# Perform Chi-square goodness-of-fit test\n",
    "chi2_stat, p_value = stats.chisquare(observed, expected)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The observed data does not fit the expected distribution.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The observed data fits the expected distribution.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Parameters\n",
    "df = 2  # degrees of freedom\n",
    "size = 1000  # number of samples\n",
    "\n",
    "# Simulate data\n",
    "data = chi2.rvs(df, size=size)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Plot theoretical PDF\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = chi2.pdf(x, df)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "title = f\"Chi-Square Distribution (df={df})\"\n",
    "plt.title(title)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 16. Implement an F-test using Python to compare the variances of two random samples\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "sample1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "sample2 = np.random.normal(loc=0, scale=2, size=100)\n",
    "\n",
    "# Perform F-test\n",
    "f_stat, p_value = stats.levene(sample1, sample2)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The variances are significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The variances are not significantly different.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 17. Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "group1 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group2 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group3 = np.random.normal(loc=0, scale=1, size=30)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: At least one group mean is significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: All group means are equal.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 18. Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "group1 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group2 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group3 = np.random.normal(loc=0, scale=1, size=30)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: At least one group mean is significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: All group means are equal.\"\n",
    "\n",
    "# Plot boxplot\n",
    "plt.boxplot([group1, group2, group3], labels=['Group 1', 'Group 2', 'Group 3'])\n",
    "plt.title('Comparison of Group Means')\n",
    "plt.ylabel('Values')\n",
    "plt.show()\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 19. Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def check_anova_assumptions(*groups):\n",
    "    \"\"\"\n",
    "    Check the assumptions for ANOVA: normality, independence, and equal variance.\n",
    "    \n",
    "    Parameters:\n",
    "    *groups: Groups of data to be tested\n",
    "    \n",
    "    Returns:\n",
    "    dict: Results of normality and equal variance tests\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Normality test (Shapiro-Wilk)\n",
    "    normality_results = [stats.shapiro(group) for group in groups]\n",
    "    results['normality'] = normality_results\n",
    "    \n",
    "    # Equal variance test (Levene's test)\n",
    "    levene_stat, levene_p = stats.levene(*groups)\n",
    "    results['equal_variance'] = (levene_stat, levene_p)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "group1 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group2 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group3 = np.random.normal(loc=0, scale=1, size=30)\n",
    "\n",
    "assumptions = check_anova_assumptions(group1, group2, group3)\n",
    "assumptions\n",
    "\n",
    "\n",
    "# 20. Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "factor1 = np.repeat(['A', 'B'], 50)\n",
    "factor2 = np.tile(['X', 'Y', 'Z'], 50)\n",
    "values = np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Factor1': factor1, 'Factor2': factor2, 'Values': values})\n",
    "\n",
    "# Fit the model\n",
    "model = ols('Values ~ C(Factor1) + C(Factor2) + C(Factor1):C(Factor2)', data=df).fit()\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if anova_table['PR(>F)']['C(Factor1):C(Factor2)'] < alpha:\n",
    "    result = \"Reject the null hypothesis: There is a significant interaction between Factor1 and Factor2.\"\n",
    "\n",
    "::contentReference[oaicite:0]{index=0}\n",
    " \n",
    "\n",
    "\n",
    "# 21. Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f\n",
    "\n",
    "# Parameters\n",
    "dfn = 2  # degrees of freedom for numerator\n",
    "dfd = 5  # degrees of freedom for denominator\n",
    "x = np.linspace(0, 5, 1000)\n",
    "\n",
    "# Probability density function\n",
    "y = f.pdf(x, dfn, dfd)\n",
    "\n",
    "# Plot\n",
    "plt.plot(x, y, label=f'F-distribution (dfn={dfn}, dfd={dfd})')\n",
    "plt.title('F-distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The F-distribution is used in hypothesis testing, particularly in ANOVA, to compare variances across groups.\n",
    "\n",
    "\n",
    "# 22. Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "group1 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group2 = np.random.normal(loc=0, scale=1, size=30)\n",
    "group3 = np.random.normal(loc=0, scale=1, size=30)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: At least one group mean is significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: All group means are equal.\"\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot([group1, group2, group3], labels=['Group 1', 'Group 2', 'Group 3'])\n",
    "plt.title('Comparison of Group Means')\n",
    "plt.ylabel('Values')\n",
    "plt.show()\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "# 23. Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "sample1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "sample2 = np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(sample1, sample2)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The means are significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The means are not significantly different.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "# 24. Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "sample = np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "# Sample variance\n",
    "s2 = np.var(sample, ddof=1)\n",
    "\n",
    "# Population variance (hypothesized)\n",
    "sigma2_0 = 1\n",
    "\n",
    "# Degrees of freedom\n",
    "df = len(sample) - 1\n",
    "\n",
    "# Chi-square statistic\n",
    "chi2_stat = df * s2 / sigma2_0\n",
    "\n",
    "# p-value\n",
    "p_value = 1 - stats.chi2.cdf(chi2_stat, df)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The sample variance is significantly different from the hypothesized population variance.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The sample variance is not significantly different from the hypothesized population variance.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "# 25. Write a Python script to perform a Z-test for comparing proportions between two datasets or groups\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Sample data\n",
    "successes = np.array([50, 60])\n",
    "totals = np.array([100, 100])\n",
    "\n",
    "# Perform Z-test\n",
    "stat, p_value = proportions_ztest(successes, totals)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The proportions are significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The proportions are not significantly different.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "# 26. Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "sample1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "sample2 = np.random.normal(loc=0, scale=2, size=100)\n",
    "\n",
    "# Variances\n",
    "var1 = np.var(sample1, ddof=1)\n",
    "var2 = np.var(sample2, ddof=1)\n",
    "\n",
    "# F-statistic\n",
    "f_stat = var1 / var2\n",
    "\n",
    "# Degrees of freedom\n",
    "df1 = len(sample1) - 1\n",
    "df2 = len(sample2) - 1\n",
    "\n",
    "# p-value\n",
    "p_value = 1 - stats.f.cdf(f_stat, df1, df2)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The variances are significantly different.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The variances are not significantly different.\"\n",
    "\n",
    "# Plot F-distribution\n",
    "x = np.linspace(0, 5, 1000)\n",
    "y = stats.f.pdf(x, df1, df2)\n",
    "plt.plot(x, y, label=f'F-distribution (df1={df1}, df2={df2})')\n",
    "plt.title('F-distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "# 27. Perform a Chi-square test for goodness of fit with simulated data and analyze the results\n",
    "# **Answer**:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate observed data\n",
    "np.random.seed(0)\n",
    "observed = np.random.choice([1, 2, 3], size=100, p=[0.2, 0.5, 0.3])\n",
    "\n",
    "# Expected frequencies\n",
    "expected = np.array([0.2, 0.5, 0.3]) * len(observed)\n",
    "\n",
    "# Perform Chi-square goodness-of-fit test\n",
    "chi2_stat, p_value = stats.chisquare(np.bincount(observed - 1), expected)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis: The observed data does not fit the expected distribution.\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis: The observed data fits the expected distribution.\"\n",
    "\n",
    "result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
